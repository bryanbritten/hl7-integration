### Objective

The goal of this project is to demonstrate my familiarity with the data, tools, and workflows that a Data Integration Engineer in the healthcare space would deal with on a day-to-day basis. 

### Supported Data

Currently, the pipeline only supports `ADT_A01` and `ADT_A03` HL7v2.5 messages, but I have plans of expanding this project to support additional HL7v2.5 messages (e.g. `ORU_R01`), as well as C-CDA and X12 data.

### Architecture - Overview

The pipeline is written almost entirely in Python and is containerized using Docker. A container is created for each of the following services:
- `broker`: A Kafka cluster that serves as the message transport layer between the various services.
- `fhir-converter`: A containerized version of Microsoft's [FHIR Converter](https://github.com/microsoft/FHIR-Converter).  
- `storage`: Uses `minio` to create containers that act as a proxy for AWS S3 storage.
- `producer`: Generates a stream of dynamically generated `ADT_A01` and `ADT_A03` messages.  
- `consumer`: Reads, parses, and validates raw messages. `ACK` messages are sent based on the outcome of parsing and validation attempts.  
- `qa`: Reads messages that have passed initial validation and performs additional data quality checks.  
- `transformer`: Converts a validated message into the FHIR format using the `fhir-converter` service.  
- `monitor`: Monitors the pipeline and aggregates performance metrics.

### Architecture - 3rd-Party Services

#### Broker

The `broker` service is a Kafka "cluster" consisting of a sinkgle broker. The topics include `hl7.ingest`, `hl7.accepted`, `hl7.validated`, `DLQ`, and `ACKS`. Here's a brief description of each topic:

- hl7.ingest: Facilitates communication between the `producer` and `consumer` services. This topic holds the raw messages generated by `producer`.  
- hl7.accepted: Facilitates communication between the `consumer` and `qa` services. Once the `consumer` service successfully parses and validates a message, it is written to the `hl7.accepted` topic.  
- hl7.validated: Facilitates communication between the `qa` and `transformer` services. If a message passes all data quality checks implemented by the `qa` service, the message is written to the `hl7.validated` topic in preparation of being converted to FHIR.  
- DLQ: The Dead Letter Queue. Any messages that cannot be parsed, fail validation, fail data quality checks, or fail to convert to FHIR are written to this topic for manual review.  
- ACKS: The acknowledgement topic, which is used to send `ACK` messages from the `consumer` service. No functionality exists to handle the `ACK` messages; they are created purely for demonstrative purposes.

> NOTE: A production-level integration engine would have more brokers in the cluster, and would set the replication factor to a value higher than one. This would add performance and durability.

#### FHIR Converter

The conversion to FHIR is done with a containerized version of the [Microsoft FHIR Converter](https://github.com/microsoft/FHIR-Converter). The image can be found [here](https://hub.docker.com/r/microsoft/healthcareapis-fhir-converter).

#### Storage

`minio` is used to replicate the use of an S3 bucket. A medallion-like architecture is utilized while processing the data. The `consumer` service ingests the raw message and saves it in a "bronze" layer, which represents data that has been accepted by the ingestion engine. The "silver" layer is utilized by the `transformer` service and serves as the final sink for the pipeline. Messages that successfully pass QA checks and convert to FHIR are placed in this bucket. This is the bucket data engineers would pull from to build "gold" layer tables, dashboards, and reports.

#### Monitor

The `monitor` service makes use of Grafana and Prometheous to monitor activity in the `producer`, `consumer`, `qa`, and `transformer` services. The following metrics are captured:
- Message send rate  
- Message ingest rate  
- Validation & QA Check fail rates  
- FHIR Conversion Success Rate

### Architecture - 1st-Party Services

#### Producer

The `producer` generates `ADT_A01` and `ADT_A03` messages dynamically by making use of the `faker` and `random` libraries in Python. The [hl7_segment_generators.py](https://github.com/bryanbritten/hl7-integration/blob/main/docker/producer/generators/hl7_segment_generators.py) file defines the functions that create the segments in a given message. The [fake_data_generators.py](https://github.com/bryanbritten/hl7-integration/blob/main/docker/producer/generators/fake_data_generators.py) file is responsible for generating the random values that go in each field of the different segments. 

The `faker` library is used to generate random values for names, addresses, SSNs, UUIDs, phone numbers, birth dates, and more. Additionally, a decorator called `with_error_rate` is used on each value generator to randomly introduce missing values. The error rate is adjustable for each value-generating function.

The `ADT_A01` and `ADT_A03` schemas were defined with the help of definitions provided by [Caristix](https://hl7-definition.caristix.com/v2/HL7v2.5/Segments).

#### Consumer

The `consumer` service is responsible for reading messages from Kafka and sending acknowledgement messages (i.e. `MSH-9.1 = ACK`) back to Kafka. On ingest, attempts are made to parse the message using the `hl7apy` library and validate that the message meets basic structural requirements. If any of those attempts fail the message is written to the `DLQ` topic and an acknowledgement with `acknowledgement_code=AE` is written to the `ACKS` topic. If all attempts are successful, the message is written to the `hl7.accepted` topic as well as the "bronze" bucket. An acknowledgement with `acknowledgement_code=AA` is sent to the `ACKS` topic.

#### QA (Quality Assurance)

The `qa` service reads from the `hl7.accepted` topic in the `broker` service and performs a series of data quality checks using a combination of the `hl7apy` library and custom logic. If the data quality checks all pass, the message is written to the `hl7.validated` topic. Otherwise, the message is written to the `DLQ` topic with the QA failures included in the headers for later inspection. 

#### Transformer

The `transformer` service reads a message from the `hl7.validated` topic and sends the data to the `convertToFhir` API endpoint in the `fhir-converter` service. Successful conversions are then stored in the "silver" layer, while failed conversions are written to the `DLQ` topic.

### How to use

Running this pipeline locally requires Docker. If you have Docker installed, follow these steps:
1. Clone this repository  
2. Navigate into the directory created by Step 1.  
3. Run `docker compose up --build` from your terminal.  

### Next steps

If this were a production environment, there are a few things that could/should be added to improve this pipeline, including:

- Testing
- Kubernetes (for scaling producer and consumer services based on need)
- Add support for `ORU_R01` messages
- Improved data quality checks
- Validation and quality quecks on the FHIR data
- Alerts and warnings defined
- Z-segments in messages
- ~~More complex messages (repeated segments, erroneous segments, missing required segments)~~ (implemented as of [4708536f](https://github.com/bryanbritten/hl7-integration/commit/4708536f307ae5348b2c92c4aa8853e3a01b89f1))
- ~~Logging~~ (implemented as of [8480be2e](https://github.com/bryanbritten/hl7-integration/commit/8480be2e0b959be5378f9510b2da116ef040fd96))
- ~~Grafana dashboard defined as code~~ (implemented as of [59024efe](https://github.com/bryanbritten/hl7-integration/commit/59024efec96922117e9304bee9b7681f30658c94))
- ~~Message queues~~ (implemented as of [81af8b1d](https://github.com/bryanbritten/hl7-integration/commit/81af8b1d62f42e6b69dc311272de68b2bdfbe230))
- ~~Add support for `ADT_A03` messages~~ (implemented as of [05dfe236](https://github.com/bryanbritten/hl7-integration/commit/05dfe236d967805fdb7199d2e18c0903c5f9d37e))


![image of a Grafana dashboard with the four metrics defined above](static/img/grafana_dashboard.png)